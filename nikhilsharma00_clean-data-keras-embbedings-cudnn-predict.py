# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from tqdm import tqdm 

from nltk.stem import WordNetLemmatizer

from nltk.corpus import stopwords

import threading

#from numpy import array
# Input data files are available in the "../input/" directory.

train = pd.read_csv("../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv")

test = pd.read_csv("../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv")



# Converting data in Data framres 

train=pd.DataFrame(train)

test=pd.DataFrame(test)
print(train.isnull().sum())

#Source:simple-eda-text-preprocessing-jigsaw

import matplotlib.pyplot as plt

import seaborn as sns

demographics = train.loc[:, ['target']+list(train)[slice(8,32)]].dropna()

weighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis="index").sum()/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()

weighted_toxic = weighted_toxic.sort_values(ascending=False)

plt.figure(figsize=(30,20))

sns.set(font_scale=3)

ax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)

plt.ylabel('Demographics')

plt.xlabel('Weighted Toxic')

plt.show()

del sns,weighted_toxic,demographics,ax
def preprocess(data):

    '''

    Reference: https://www.kaggle.com/abhigupta4981/pytorch-train-with-callbacks

               and

               simple-eda-text-preprocessing-jigsaw

    '''

    print("pre-processing")

    punct = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']',

          '>', '%', '=', '#', '*', '+', '\\', 'â€¢', '~', '@', 'Â£', 'Â·', '_', '{', '}', 'Â©', '^','\n'

          'Â®', '`', '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º', 'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ',

          'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶',

          'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼',

          'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²',

          'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª',

          'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš']

    

    def clean_special_chars(text, punct):

        def get_misspell(text):

            

            contract = {"'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ðŸ˜‰':'wink','ðŸ˜‚':'joy','ðŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',Trump's" : 'trump is',"'cause": 'because',',cause': 'because',';cause': 'because',"ain't": 'am not','ain,t': 'am not',

    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',"aren't": 'are not',

    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',"can't": 'cannot',"can't've": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',

    'can;t': 'cannot','can;t;ve': 'cannot have','canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',

    "could've": 'could have','could,ve': 'could have','could;ve': 'could have',"couldn't": 'could not',"couldn't've": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',

    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',

    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',

    'couldâ€™ve': 'could have',"didn't": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',

    'didnâ€™t': 'did not',"doesn't": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',

    'doesnâ€™t': 'does not',"don't": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',

    "hadn't": 'had not',"hadn't've": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',

    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',"hasn't": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',

    "haven't": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',"he'd": 'he would',

    "he'd've": 'he would have',"he'll": 'he will',

    "he's": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',

    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',

    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',"how'd": 'how did',"how'll": 'how will',

    "how's": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',

    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',

    'howâ€™s': 'how is',"i'd": 'i would',"i'll": 'i will',"i'm": 'i am',"i've": 'i have','i,d': 'i would','i,ll': 'i will',

    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',"isn't": 'is not',

    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',"it'd": 'it would',"it'll": 'it will',"It's":'it is',

    "it's": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',

    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',

    'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',

    'iâ€™ve': 'i have',"let's": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',

    'letâ€™s': 'let us',"ma'am": 'madam','ma,am': 'madam','ma;am': 'madam',"mayn't": 'may not','mayn,t': 'may not','mayn;t': 'may not',

    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',"might've": 'might have','might,ve': 'might have','might;ve': 'might have',"mightn't": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',

    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',"must've": 'must have','must,ve': 'must have','must;ve': 'must have',

    "mustn't": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',

    'mustâ€™ve': 'must have',"needn't": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',"oughtn't": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',

    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',"sha'n't": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',"shan't": 'shall not',

    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',

    "she'd": 'she would',"she'll": 'she will',"she's": 'she is','she,d': 'she would','she,ll': 'she will',

    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',

    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',"should've": 'should have','should,ve': 'should have','should;ve': 'should have',

    "shouldn't": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',

    'shouldâ€™ve': 'should have',"that'd": 'that would',"that's": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',

    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',"there'd": 'there had',

    "there's": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',

    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',

    "they'd": 'they would',"they'll": 'they will',"they're": 'they are',"they've": 'they have',

    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',

    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',

    'theyâ€™re': 'they are','theyâ€™ve': 'they have',"wasn't": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',

    'wasnâ€™t': 'was not',"we'd": 'we would',"we'll": 'we will',"we're": 'we are',"we've": 'we have','we,d': 'we would','we,ll': 'we will',

    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',

    "weren't": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',

    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',"what'll": 'what will',"what're": 'what are',"what's": 'what is',

    "what've": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',

    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',

    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',

    'whatâ€™ve': 'what have',"where'd": 'where did',"where's": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',

    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',

    "who'll": 'who will',"who's": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',

    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',"won't": 'will not','won,t': 'will not','won;t': 'will not',

    'wonÂ´t': 'will not','wonâ€™t': 'will not',"wouldn't": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',

    'wouldnâ€™t': 'would not',"you'd": 'you would',"you'll": 'you will',"you're": 'you are','you,d': 'you would','you,ll': 'you will',

    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',

    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',

    'Â´cause': 'because','â€™cause': 'because',"you've": "you have","could'nt": 'could not',

    "havn't": 'have not',"hereâ€™s": "here is",'i""m': 'i am',"i'am": 'i am',"i'l": "i will","i'v": 'i have',"wan't": 'want',"was'nt": "was not","who'd": "who would",

    "who're": "who are","who've": "who have","why'd": "why would","would've": "would have","y'all": "you all","y'know": "you know","you.i": "you i",

    "your'e": "you are","arn't": "are not","agains't": "against","c'mon": "common","doens't": "does not",'don""t': "do not","dosen't": "does not",

    "dosn't": "does not","shoudn't": "should not","that'll": "that will","there'll": "there will","there're": "there are",

    "this'll": "this all","u're": "you are", "ya'll": "you all","you'r": "you are","youâ€™ve": "you have","d'int": "did not","did'nt": "did not","din't": "did not","dont't": "do not","gov't": "government",

    "i'ma": "i am","is'nt": "is not","â€˜I":'I',

    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\

    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\

    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\

    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\

    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\

    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\

    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',

    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',

    }

            for word in text.split():

                if word.lower() in contract:

                    text = text.replace(word, contract[word.lower()])

            return text

        text=get_misspell(text)    

        for p in (punct):

            text = text.replace(p,' ')

        return text

    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))



    return data

# Pre-proccesing train data

data=preprocess(train['comment_text'].astype(str))

test_data=preprocess(test['comment_text'].astype(str))
# Tokenize_lammatize and Stop Words removal

from keras.preprocessing.text import Tokenizer

Token=Tokenizer()

Total=data

Total.append(test_data) #Total Data available for Vocab

Token.fit_on_texts(Total)

sequence = Token.texts_to_sequences(data)

vocab_size = len(sequence)+1
vocab_size
# Check All Loaded files and imports in RAM

print(dir())

import gc
#Loading Keras for training 



from keras.models import Sequential

from keras.layers import Dense,Embedding,CuDNNLSTM

from keras.layers import Convolution1D, GlobalMaxPooling1D,GlobalAveragePooling1D

from keras.layers import Bidirectional

from keras.preprocessing.sequence import pad_sequences

gc.collect()
# Loading Glove Model



f = open("../input/glove6b/glove.6B.100d.txt",'r') # Load Model

embedding_matrix = np.zeros((1804875,100))

embedding_values = {}

for line in tqdm(f):

    value = line.split(' ')

    word = value[0]

    coef = np.array(value[1:],dtype = 'float32')

    embedding_values[word]=coef

print("Model Loaded")



# Padding Sequence Which are nothing just integer value Encoded sentences

pad_seq = pad_sequences(sequence,maxlen = 100)



# Preparing Matrix for word Embeddings out of Vocab used

for word,i in tqdm(Token.word_index.items()):

    values = embedding_values.get(word)

    if values is not None:

        embedding_matrix[i] = values
"""https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution"""

identity_columns = ['asian', 'atheist',

       'bisexual', 'black', 'buddhist', 'christian', 'female',

       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',

       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',

       'muslim', 'other_disability', 'other_gender',

       'other_race_or_ethnicity', 'other_religion',

       'other_sexual_orientation', 'physical_disability',

       'psychiatric_or_mental_illness', 'transgender', 'white']

# Overall

weights = np.ones((len(train),)) / 4

# Subgroup

weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4

# Background Positive, Subgroup Negative

weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +

   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4

# Background Negative, Subgroup Positive

weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +

   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4

loss_weight = 1.0 / weights.mean()

y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T




# Sentence Via Biderectional LSTM

model=Sequential()

model.add(Embedding(1804875,100,input_length = 100,weights = [embedding_matrix],trainable = False))

model.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))

model.add(GlobalAveragePooling1D())

model.add(Dense(128,activation = 'relu'))

model.add(Dense(2,activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

model.summary()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(pad_seq,y_train, test_size=0.25, random_state=42)
history=model.fit(X_train,y_train,epochs=10,batch_size=1000,validation_data=(X_test,y_test))

plt.plot(history.history['acc'])

plt.plot(history.history['val_acc'])

plt.title('Model accuracy')

plt.ylabel('Accuracy')

plt.xlabel('Epoch')

plt.legend(['Train', 'Test'], loc='upper left')

plt.show()

# Plot training & validation loss values

plt.plot(history.history['loss'])

plt.plot(history.history['val_loss'])

plt.title('Model loss')

plt.ylabel('Loss')

plt.xlabel('Epoch')

plt.legend(['Train', 'Test'], loc='upper left')

plt.show()
del pad_seq,X_train, X_test, y_train, y_test

sequence = Token.texts_to_sequences(test_data)

test_pad = pad_sequences(sequence,maxlen = 100)

prediction = model.predict(test_pad)

del test_data,Total,Token,embedding_matrix,embedding_values
#Alining the values to submit

#Submiting the results

submission = pd.DataFrame([test['id']]).T

submission['prediction'] = [float(x) for x,_ in prediction]
submission.to_csv('submission.csv', index=False)

submission.head()